{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file handling\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import unidecode\n",
    "import sys\n",
    "\n",
    "# image handling\n",
    "import base64\n",
    "\n",
    "# AI API\n",
    "import anthropic\n",
    "\n",
    "# parsing\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# tabular data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# date handling\n",
    "import locale\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined functions\n",
    "# Set path for scripts folder\n",
    "path_scripts = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(path_scripts)\n",
    "from logs import custom_logging as clogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = clogs.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup constructor\n",
    "api_key = os.getenv(\"CLAUDE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"CLAUDE_API_KEY not found\")\n",
    "client = anthropic.Anthropic(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use AI API to extract text from image\n",
    "def extract_img2text(image_path, prompt):\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=3200,\n",
    "        messages=[{\n",
    "            \"role\":\n",
    "            \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/jpeg\",\n",
    "                    \"data\": image_data,\n",
    "                },\n",
    "            }, {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }],\n",
    "        }],\n",
    "    )\n",
    "    return message\n",
    "\n",
    "# parse csv string to list of lists\n",
    "def parse_csv_string(csv_string):\n",
    "    # Remove the leading newline and split the string into lines\n",
    "    lines = csv_string.strip().split('\\n')\n",
    "\n",
    "    # Parse the CSV data\n",
    "    reader = csv.reader(StringIO('\\n'.join(lines)))\n",
    "\n",
    "    # Convert to list of lists\n",
    "    data = list(reader)\n",
    "    # return output\n",
    "    return data\n",
    "\n",
    "def insert_column_name(df, column_name):\n",
    "    # Get the index of the specified column\n",
    "    col_index = df.columns.get_loc(column_name)\n",
    "\n",
    "    # Insert a new column with the column name as the constant value\n",
    "    df.insert(col_index, f'{column_name}_name', column_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_date(date_string, year):\n",
    "    # Set locale to Spanish\n",
    "    locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "    # Parse the date string\n",
    "    date_obj = dt.datetime.strptime(f\"{date_string} {year}\", \"%B %A %d %Y\")\n",
    "\n",
    "    # Reset locale\n",
    "    locale.setlocale(locale.LC_TIME, '')\n",
    "\n",
    "    # format: 2/03/2024\n",
    "    return date_obj.strftime('%-d/%m/%Y')\n",
    "\n",
    "\n",
    "def normalize_day(input_string):\n",
    "    day_mapping = {\n",
    "        \"Lun\":\"Lunes\",\n",
    "        \"Mar\": \"Martes\",\n",
    "        \"Mié\": \"Miércoles\",\n",
    "        \"Mie\": \"Miércoles\",\n",
    "        \"Jue\": \"Jueves\",\n",
    "        \"Vie\": \"Viernes\",\n",
    "        \"Sab\": \"Sábado\",\n",
    "        \"Sáb\": \"Sábado\",\n",
    "        \"Dom\": \"Domingo\"\n",
    "    }\n",
    "\n",
    "    # Convert to title case and split\n",
    "    parts = input_string.title().split()\n",
    "\n",
    "    # Check if the second part is a day abbreviation\n",
    "    if len(parts) > 1:\n",
    "        day_abbr = parts[1][:3]  # Take first 3 characters\n",
    "        if day_abbr in day_mapping:\n",
    "            parts[1] = day_mapping[day_abbr]\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def normalize_month(input_string):\n",
    "\n",
    "    month_mapping = {\n",
    "        \"Ene\": \"Enero\",\n",
    "        \"Feb\": \"Febrero\",\n",
    "        \"Mar\": \"Marzo\",\n",
    "        \"Apr\": \"Abril\",\n",
    "        \"May\": \"Mayo\",\n",
    "        \"Jun\": \"Junio\",\n",
    "        \"Jul\": \"Julio\",\n",
    "        \"Aug\": \"Agosto\",\n",
    "        \"Sep\": \"Septiembre\",\n",
    "        \"Oct\": \"Octubre\",\n",
    "        \"Nov\": \"Noviembre\",\n",
    "        \"Dic\": \"Diciembre\"\n",
    "    }\n",
    "\n",
    "    # Convert to title case and split\n",
    "    parts = input_string.title().split()\n",
    "\n",
    "    # Check if the second part is a day abbreviation\n",
    "    if len(parts) > 1:\n",
    "        month_abbr = parts[0][:3]  # Take first 3 characters\n",
    "        if month_abbr in month_mapping:\n",
    "            parts[0] = month_mapping[month_abbr]\n",
    "\n",
    "    return \" \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder structure\n",
    "\n",
    "# Get the current working directory where script is located\n",
    "base_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# data folder\n",
    "data = \"_data\"\n",
    "# target folder\n",
    "batch = \"01_2025_03\"\n",
    "\n",
    "# Join paths starting from the _imgs folder\n",
    "folder_img_path = os.path.join(base_dir, data, batch, \"1_img\")\n",
    "folder_sg_path = os.path.join(base_dir, data, batch, \"2_sg_excel\")\n",
    "folder_output = os.path.join(base_dir, data, batch, \"3_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/manuel/gdrive/prgrmmng/fundo_vt/form2tab'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 73 entries, 0 to 72\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Número animal  73 non-null     object\n",
      " 1   Fecha Parto    73 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Número animal</th>\n",
       "      <th>Fecha Parto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321/5</td>\n",
       "      <td>1/03/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0836/3</td>\n",
       "      <td>28/03/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Número animal Fecha Parto\n",
       "0         321/5   1/03/2024\n",
       "1        0836/3  28/03/2024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the file matching the pattern\n",
    "file_path = glob.glob(os.path.join(folder_sg_path, \"[Ff]echa*[Pp]arto*.xlsx\"))[0]\n",
    "# Read the matched file\n",
    "data_sg = pd.read_excel(file_path, header=1)\n",
    "data_sg = data_sg.rename(columns={\n",
    "    \"Número\": \"Número animal\",\n",
    "    \"F.Últ.Par\": \"Fecha Parto\"\n",
    "}).copy()\n",
    "data_sg = data_sg[[\"Número animal\", \"Fecha Parto\"]].dropna().copy()\n",
    "\n",
    "# format: 2/03/2024\n",
    "data_sg[\"Fecha Parto\"] = data_sg[\"Fecha Parto\"].dt.strftime('%-d/%m/%Y')\n",
    "data_sg.info()\n",
    "data_sg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_level = 99.75\n",
    "prompt_input = f\"\"\"Instruction 1: Convert the text in the image to csv.\n",
    "Instruction 2: Employ a strict approach: add 1 asterisk next to the estimated values for those cells whose text-to-digit conversion are below a {conf_level} percent confidence threshold; it does not matter if data is over-flagged.\n",
    "Instruction 3: Include in comments the confidence threshold used.\n",
    "Instruction 4: Do not use outlier-detection as criteria to flag the data.\n",
    "Instruction 5: Make sure to not use outlier-detection as criteria to flag the data.\n",
    "Instruction 6: If headers are present, include them. If no headers are found, do not include any.\n",
    "Instruction 7: Include any comments before returning output. Limit verbosity.\n",
    "Instruction 8: Return output enclosed in brackets to facilitate parsing.\n",
    "Instruction 9: Do not include any additional comments after final output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'Escaner_20250119_1.jpg', 'Escaner_20250119_2.jpg', 'Escaner_20250119_3.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(os.listdir(folder_img_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 12:51:02 - INFO - Processing file: Escaner_20250119_1.jpg\n",
      "2025-01-22 12:51:02 - DEBUG - cols_list initialized: []\n",
      "2025-01-22 12:51:25 - INFO - API Comment: Comments:\n",
      "- Confidence threshold used: 99.75%\n",
      "- Asterisks (*) indicate values with confidence below 99.75%\n",
      "- Headers are included as they are present in the image\n",
      "2025-01-22 12:51:25 - DEBUG - cols_list initialized: ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:51:25 - INFO - Dataframe successfully created\n",
      "2025-01-22 12:51:25 - DEBUG - cols_list at end of iteration: ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:51:25 - DEBUG - data_df columns: ['Número animal', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count']\n",
      "2025-01-22 12:51:25 - DEBUG - data_final columns: ['Número animal', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count', 'Fecha Parto']\n",
      "2025-01-22 12:51:25 - DEBUG -  columns: ['Número animal', 'Fecha Parto', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count']\n",
      "2025-01-22 12:51:25 - DEBUG - ---\n",
      "2025-01-22 12:51:25 - INFO - Processing file: Escaner_20250119_2.jpg\n",
      "2025-01-22 12:51:25 - DEBUG - cols_list initialized: ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:51:48 - INFO - API Comment: Comments:\n",
      "- Confidence threshold used: 99.75%\n",
      "- Asterisks (*) indicate values with less than 99.75% confidence in text-to-digit conversion\n",
      "- Headers included as they are present in the image\n",
      "- No outlier detection used for flagging\n",
      "2025-01-22 12:51:48 - DEBUG - cols_list current (no update): ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:51:48 - INFO - Dataframe successfully created\n",
      "2025-01-22 12:51:49 - DEBUG - cols_list at end of iteration: ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:51:49 - DEBUG - data_df columns: ['Número animal', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count']\n",
      "2025-01-22 12:51:49 - DEBUG - data_final columns: ['Número animal', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count', 'Fecha Parto']\n",
      "2025-01-22 12:51:49 - DEBUG -  columns: ['Número animal', 'Fecha Parto', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count']\n",
      "2025-01-22 12:51:49 - DEBUG - ---\n",
      "2025-01-22 12:51:49 - INFO - Processing file: Escaner_20250119_3.jpg\n",
      "2025-01-22 12:51:49 - DEBUG - cols_list initialized: ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:52:08 - INFO - API Comment: Comments: Using a 99.75% confidence threshold for text-to-digit conversion. No outlier detection was used for flagging.\n",
      "2025-01-22 12:52:08 - DEBUG - cols_list current (no update): ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:52:08 - INFO - Dataframe successfully created\n",
      "2025-01-22 12:52:08 - DEBUG - cols_list at end of iteration: ['Número vaca', 'Nombre', 'Becerro', 'Ene. Lunes 13', 'Ene. Martes 14', 'Ene. Miérc. 15', 'Ene. Jueves 16', 'Ene. Vierne 17', 'Ene. Sáb. 18', 'Ene. Dom. 19', '#']\n",
      "2025-01-22 12:52:08 - DEBUG - data_df columns: ['Número animal', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count']\n",
      "2025-01-22 12:52:08 - DEBUG - data_final columns: ['Número animal', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count', 'Fecha Parto']\n",
      "2025-01-22 12:52:08 - DEBUG -  columns: ['Número animal', 'Fecha Parto', 'Fecha 1', 'Kg/Leche', 'Fecha 2', 'Kg/Leche', 'Fecha 3', 'Kg/Leche', 'Fecha 4', 'Kg/Leche', 'Fecha 5', 'Kg/Leche', 'Fecha 6', 'Kg/Leche', 'Fecha 7', 'Kg/Leche', 'flag_count']\n",
      "2025-01-22 12:52:08 - DEBUG - ---\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "cols_list = []\n",
    "year = 2025\n",
    "\n",
    "class NoColsError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "for filename in sorted(os.listdir(folder_img_path)):\n",
    "    if filename.endswith(('.jpeg', '.jpg')):\n",
    "        clogs.log_file_processing(logger, filename)\n",
    "        clogs.log_column_status(logger, \"initialized\", cols_list)\n",
    "\n",
    "        image_path = os.path.join(folder_img_path, filename)\n",
    "\n",
    "        try:\n",
    "            result = extract_img2text(image_path, prompt_input)\n",
    "            clogs.log_api_comment(logger, result.content[0].text)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            continue  # Skip to next file if there's an error\n",
    "\n",
    "        data_string = result.content[0].text.split(\"[\")[1].replace(\"]\", \"\")\n",
    "        parsed_data = parse_csv_string(data_string)\n",
    "\n",
    "        if not cols_list:\n",
    "            if any(\"vaca\" in s.lower() for s in parsed_data[0]):\n",
    "                cols_list = parsed_data[0]\n",
    "                clogs.log_column_status(logger, \"initialized\", cols_list)\n",
    "            else:\n",
    "                clogs.log_validation_error(logger, parsed_data[0], \"No 'vaca' found\")\n",
    "                raise NoColsError(\"No columns found: Check image folder\")\n",
    "        else:\n",
    "            if any(\"vaca\" in s.lower()\n",
    "                   for s in parsed_data[0]) and (cols_list != parsed_data[0]):\n",
    "                cols_list = parsed_data[0]\n",
    "                clogs.log_column_status(logger, \"updated\", cols_list)\n",
    "            else:\n",
    "                clogs.log_column_status(logger, \"current (no update)\", cols_list)\n",
    "\n",
    "        # Create the DataFrame\n",
    "        try:\n",
    "            data_df = pd.DataFrame(parsed_data[1:], columns=cols_list)\n",
    "            clogs.log_dataframe_creation(logger, success=True)\n",
    "        except Exception as e:\n",
    "            clogs.log_dataframe_creation(logger, success=False, error=str(e))\n",
    "            clogs.log_column_list(logger, cols_list, \"Current columns\")\n",
    "            \n",
    "            for c in parsed_data[1:]:\n",
    "                if len(c) > len(cols_list):\n",
    "                    logger.error(f\"Row longer than columns: {c}\")\n",
    "                    break\n",
    "            \n",
    "            logger.warning(\"Breaking processing loop due to DataFrame creation error\")\n",
    "            break\n",
    "\n",
    "        clogs.log_column_status(logger, \"at end of iteration\", cols_list)\n",
    "\n",
    "        data_df['flag_count'] = data_df.apply(\n",
    "            lambda row: row.astype(str).str.count('\\*').sum(), axis=1)\n",
    "\n",
    "        col_label_num = 1\n",
    "        for col in data_df.iloc[:, 3:10].columns.tolist():\n",
    "\n",
    "            data_df[col] = data_df[col].astype(str).str.replace(\"-*\", \"\").str.replace(\"-\", \"\")\n",
    "\n",
    "            # Get the index of the specified column\n",
    "            col_index = data_df.columns.get_loc(col)\n",
    "\n",
    "            col_label_str = normalize_month(normalize_day(col.replace(\".\",\n",
    "                                                                      \"\")))\n",
    "\n",
    "            # Insert a new column with the column name as the constant value\n",
    "            data_df.insert(col_index, f'Fecha {col_label_num}',\n",
    "                           convert_to_date(col_label_str, year = year))\n",
    "            col_label_num += 1\n",
    "            data_df = data_df.rename(columns={col: \"Kg/Leche\"}).copy()\n",
    "\n",
    "        data_df = data_df.drop(columns=[\"Nombre\", \"Becerro\", \"Fecha PP\", \"#\"],\n",
    "                               errors=\"ignore\").copy()\n",
    "\n",
    "        data_df = data_df.rename(columns={\n",
    "            data_df.columns[0]: \"Número animal\"\n",
    "        }).copy()\n",
    "\n",
    "        data_df[\"Número animal\"] = data_df[\"Número animal\"].str.replace(\n",
    "            \"-\", \"/\").copy()\n",
    "        clogs.log_dataframe_columns(logger, \"data_df\", data_df.columns.tolist())\n",
    "        data_final = data_df.merge(data_sg, on=\"Número animal\", how=\"left\")\n",
    "        clogs.log_dataframe_columns(logger, \"data_final\", data_final.columns.tolist())\n",
    "        data_final[\"Fecha Parto\"] = data_final[\"Fecha Parto\"].fillna(\n",
    "            \"X*\").copy()\n",
    "\n",
    "        # Get the name of the column you want to move\n",
    "        cols_to_move = [\"Número animal\", \"Fecha Parto\"]\n",
    "\n",
    "        for col in reversed(cols_to_move):\n",
    "            data_final.insert(0, col, data_final.pop(col))\n",
    "\n",
    "        clogs.log_column_list(logger, data_final.columns.tolist())\n",
    "        data_list.append(data_final)\n",
    "        clogs.log_process_separator(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(os.path.join(folder_output,\n",
    "                                 f\"leche_{batch}.xlsx\")) as writer:\n",
    "    for i, df in enumerate(data_list):\n",
    "        df.to_excel(writer, sheet_name=f'Sheet_{i+1}', index=False)\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(folder_output,\n",
    "                                 f\"leche_{batch}_final.xlsx\")) as writer:\n",
    "    for i, df in enumerate(data_list):\n",
    "        df.to_excel(writer, sheet_name=f'Sheet_{i+1}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_scikit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
